{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QEIR Framework - Advanced Configuration and Customization\n",
    "\n",
    "This notebook demonstrates advanced configuration options and customization techniques for the QEIR framework.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. **Advanced Configuration Options**\n",
    "2. **Custom Time Periods and QE Episodes**\n",
    "3. **Alternative Data Series and Proxies**\n",
    "4. **Model Parameter Tuning**\n",
    "5. **Robustness Testing Configuration**\n",
    "6. **Performance Optimization**\n",
    "7. **Custom Analysis Workflows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from qeir.core.hypothesis_testing import QEHypothesisTester, HypothesisTestingConfig\n",
    "from qeir.utils.hypothesis_data_collector import HypothesisDataCollector\n",
    "from qeir.utils.api_error_handler import RetryConfig\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "print(\"Advanced Configuration Tutorial - Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Configuration Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: High-precision research configuration\n",
    "research_config = HypothesisTestingConfig(\n",
    "    # Extended time period\n",
    "    start_date=\"2000-01-01\",\n",
    "    end_date=\"2023-12-31\",\n",
    "    \n",
    "    # High precision settings\n",
    "    confidence_level=0.99,  # 99% confidence intervals\n",
    "    bootstrap_iterations=2000,  # More bootstrap iterations\n",
    "    \n",
    "    # Hypothesis 1: Threshold effects\n",
    "    h1_threshold_trim=0.10,  # More aggressive trimming\n",
    "    h1_min_regime_size=15,   # Larger minimum regime size\n",
    "    h1_test_alternative_thresholds=True,\n",
    "    h1_confidence_proxy='financial_stress_index',  # Alternative proxy\n",
    "    \n",
    "    # Hypothesis 2: Investment effects\n",
    "    h2_max_horizon=24,  # Longer horizon (2 years)\n",
    "    h2_lags=6,          # More lags\n",
    "    h2_use_instrumental_variables=True,\n",
    "    h2_investment_measure='equipment_investment',  # Focus on equipment\n",
    "    \n",
    "    # Hypothesis 3: International effects\n",
    "    h3_causality_lags=6,\n",
    "    h3_test_spillovers=True,\n",
    "    h3_exchange_rate_measure='eur_usd_rate',  # Focus on EUR/USD\n",
    "    \n",
    "    # Comprehensive robustness testing\n",
    "    enable_robustness_tests=True,\n",
    "    test_alternative_periods=True,\n",
    "    test_alternative_specifications=True,\n",
    "    \n",
    "    # Output settings\n",
    "    generate_publication_outputs=True,\n",
    "    save_intermediate_results=True,\n",
    "    output_directory=\"advanced_results\"\n",
    ")\n",
    "\n",
    "print(\"Research Configuration:\")\n",
    "print(f\"  Time period: {research_config.start_date} to {research_config.end_date}\")\n",
    "print(f\"  Bootstrap iterations: {research_config.bootstrap_iterations}\")\n",
    "print(f\"  Confidence level: {research_config.confidence_level}\")\n",
    "print(f\"  H2 max horizon: {research_config.h2_max_horizon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Fast prototyping configuration\n",
    "prototype_config = HypothesisTestingConfig(\n",
    "    # Shorter time period for speed\n",
    "    start_date=\"2015-01-01\",\n",
    "    end_date=\"2020-12-31\",\n",
    "    \n",
    "    # Fast settings\n",
    "    confidence_level=0.90,  # Lower confidence level\n",
    "    bootstrap_iterations=50,  # Minimal bootstrap\n",
    "    \n",
    "    # Simplified model settings\n",
    "    h1_threshold_trim=0.20,  # Conservative trimming\n",
    "    h2_max_horizon=8,        # Shorter horizon\n",
    "    h2_lags=2,               # Fewer lags\n",
    "    h2_use_instrumental_variables=False,  # Skip IV\n",
    "    h3_causality_lags=2,\n",
    "    h3_test_spillovers=False,  # Skip spillover tests\n",
    "    \n",
    "    # Minimal robustness testing\n",
    "    enable_robustness_tests=False,\n",
    "    save_intermediate_results=False\n",
    ")\n",
    "\n",
    "print(\"\\nPrototype Configuration:\")\n",
    "print(f\"  Time period: {prototype_config.start_date} to {prototype_config.end_date}\")\n",
    "print(f\"  Bootstrap iterations: {prototype_config.bootstrap_iterations}\")\n",
    "print(f\"  Robustness tests: {prototype_config.enable_robustness_tests}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Time Periods and QE Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specific QE episodes\n",
    "qe_episodes = {\n",
    "    'QE1': ('2008-11-25', '2010-06-30'),\n",
    "    'QE2': ('2010-11-03', '2011-06-30'),\n",
    "    'QE3': ('2012-09-13', '2014-10-29'),\n",
    "    'COVID_QE': ('2020-03-15', '2021-12-31'),\n",
    "    'Pre_Crisis': ('2000-01-01', '2007-12-31'),\n",
    "    'Post_Crisis': ('2015-01-01', '2019-12-31')\n",
    "}\n",
    "\n",
    "# Function to create episode-specific configurations\n",
    "def create_episode_config(episode_name, start_date, end_date, base_config=None):\n",
    "    if base_config is None:\n",
    "        base_config = HypothesisTestingConfig()\n",
    "    \n",
    "    # Create new config with episode dates\n",
    "    episode_config = HypothesisTestingConfig(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        **{k: v for k, v in base_config.__dict__.items() \n",
    "           if k not in ['start_date', 'end_date']}\n",
    "    )\n",
    "    \n",
    "    # Adjust settings based on episode length\n",
    "    episode_length = (pd.to_datetime(end_date) - pd.to_datetime(start_date)).days\n",
    "    \n",
    "    if episode_length < 730:  # Less than 2 years\n",
    "        episode_config.bootstrap_iterations = 100\n",
    "        episode_config.h2_max_horizon = 8\n",
    "        episode_config.enable_robustness_tests = False\n",
    "    \n",
    "    episode_config.output_directory = f\"results_{episode_name.lower()}\"\n",
    "    \n",
    "    return episode_config\n",
    "\n",
    "# Create configurations for each episode\n",
    "episode_configs = {}\n",
    "for episode_name, (start, end) in qe_episodes.items():\n",
    "    episode_configs[episode_name] = create_episode_config(episode_name, start, end)\n",
    "    print(f\"{episode_name}: {start} to {end} ({(pd.to_datetime(end) - pd.to_datetime(start)).days} days)\")\n",
    "\n",
    "print(f\"\\nCreated {len(episode_configs)} episode-specific configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Alternative Data Series and Proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Testing different proxy combinations\n",
    "proxy_combinations = [\n",
    "    {\n",
    "        'name': 'Standard',\n",
    "        'h1_confidence_proxy': 'consumer_confidence',\n",
    "        'h1_reaction_proxy': 'fed_total_assets',\n",
    "        'h2_investment_measure': 'private_fixed_investment',\n",
    "        'h2_distortion_proxy': 'corporate_bond_spreads',\n",
    "        'h3_exchange_rate_measure': 'trade_weighted_dollar',\n",
    "        'h3_inflation_measure': 'cpi_all_items'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Alternative_1',\n",
    "        'h1_confidence_proxy': 'financial_stress_index',\n",
    "        'h1_reaction_proxy': 'monetary_base',\n",
    "        'h2_investment_measure': 'equipment_investment',\n",
    "        'h2_distortion_proxy': 'liquidity_premium',\n",
    "        'h3_exchange_rate_measure': 'eur_usd_rate',\n",
    "        'h3_inflation_measure': 'pce_price_index'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Alternative_2',\n",
    "        'h1_confidence_proxy': 'business_confidence',\n",
    "        'h1_reaction_proxy': 'fed_treasury_holdings',\n",
    "        'h2_investment_measure': 'structures_investment',\n",
    "        'h2_distortion_proxy': 'mortgage_spreads',\n",
    "        'h3_exchange_rate_measure': 'jpy_usd_rate',\n",
    "        'h3_inflation_measure': 'import_price_index'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to create proxy-specific configurations\n",
    "def create_proxy_config(proxy_dict, base_config=None):\n",
    "    if base_config is None:\n",
    "        base_config = HypothesisTestingConfig()\n",
    "    \n",
    "    # Update base config with proxy settings\n",
    "    config_dict = base_config.__dict__.copy()\n",
    "    config_dict.update({k: v for k, v in proxy_dict.items() if k != 'name'})\n",
    "    config_dict['output_directory'] = f\"results_{proxy_dict['name'].lower()}\"\n",
    "    \n",
    "    return HypothesisTestingConfig(**config_dict)\n",
    "\n",
    "# Create proxy-specific configurations\n",
    "proxy_configs = {}\n",
    "for proxy_combo in proxy_combinations:\n",
    "    config_name = proxy_combo['name']\n",
    "    proxy_configs[config_name] = create_proxy_config(proxy_combo)\n",
    "    print(f\"{config_name} configuration created\")\n",
    "    print(f\"  Confidence proxy: {proxy_combo['h1_confidence_proxy']}\")\n",
    "    print(f\"  Investment measure: {proxy_combo['h2_investment_measure']}\")\n",
    "    print(f\"  Exchange rate: {proxy_combo['h3_exchange_rate_measure']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter sensitivity analysis setup\n",
    "parameter_grids = {\n",
    "    'threshold_trim': [0.10, 0.15, 0.20, 0.25],\n",
    "    'min_regime_size': [8, 10, 12, 15],\n",
    "    'max_horizon': [12, 16, 20, 24],\n",
    "    'lags': [2, 3, 4, 6],\n",
    "    'causality_lags': [2, 3, 4, 6]\n",
    "}\n",
    "\n",
    "# Function to create parameter grid configurations\n",
    "def create_parameter_configs(base_config=None):\n",
    "    if base_config is None:\n",
    "        base_config = HypothesisTestingConfig()\n",
    "    \n",
    "    configs = []\n",
    "    \n",
    "    # Create configurations for threshold trimming sensitivity\n",
    "    for trim in parameter_grids['threshold_trim']:\n",
    "        config = HypothesisTestingConfig(\n",
    "            **{**base_config.__dict__, \n",
    "               'h1_threshold_trim': trim,\n",
    "               'bootstrap_iterations': 100,  # Reduced for speed\n",
    "               'enable_robustness_tests': False,\n",
    "               'output_directory': f'sensitivity_trim_{trim:.2f}'}\n",
    "        )\n",
    "        configs.append(('threshold_trim', trim, config))\n",
    "    \n",
    "    # Create configurations for horizon sensitivity\n",
    "    for horizon in parameter_grids['max_horizon']:\n",
    "        config = HypothesisTestingConfig(\n",
    "            **{**base_config.__dict__, \n",
    "               'h2_max_horizon': horizon,\n",
    "               'bootstrap_iterations': 100,\n",
    "               'enable_robustness_tests': False,\n",
    "               'output_directory': f'sensitivity_horizon_{horizon}'}\n",
    "        )\n",
    "        configs.append(('max_horizon', horizon, config))\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# Create parameter sensitivity configurations\n",
    "param_configs = create_parameter_configs()\n",
    "print(f\"Created {len(param_configs)} parameter sensitivity configurations\")\n",
    "\n",
    "for param_type, param_value, config in param_configs[:5]:  # Show first 5\n",
    "    print(f\"  {param_type} = {param_value}: {config.output_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced API retry configuration\n",
    "robust_retry_config = RetryConfig(\n",
    "    max_retries=5,\n",
    "    base_delay=2.0,\n",
    "    max_delay=120.0,\n",
    "    exponential_base=2.0,\n",
    "    jitter=True  # Add randomness to avoid thundering herd\n",
    ")\n",
    "\n",
    "# Fast retry configuration for development\n",
    "fast_retry_config = RetryConfig(\n",
    "    max_retries=2,\n",
    "    base_delay=0.5,\n",
    "    max_delay=10.0,\n",
    "    exponential_base=1.5\n",
    ")\n",
    "\n",
    "print(\"API Retry Configurations:\")\n",
    "print(f\"Robust: {robust_retry_config.max_retries} retries, {robust_retry_config.max_delay}s max delay\")\n",
    "print(f\"Fast: {fast_retry_config.max_retries} retries, {fast_retry_config.max_delay}s max delay\")\n",
    "\n",
    "# Example of using custom retry config\n",
    "# collector = HypothesisDataCollector(\n",
    "#     fred_api_key=os.getenv('FRED_API_KEY'),"\n",
    "#     retry_config=robust_retry_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-optimized configuration\n",
    "memory_optimized_config = HypothesisTestingConfig(\n",
    "    start_date=\"2010-01-01\",  # Shorter period\n",
    "    end_date=\"2020-12-31\",\n",
    "    bootstrap_iterations=100,  # Fewer iterations\n",
    "    h2_max_horizon=12,         # Shorter horizon\n",
    "    enable_robustness_tests=False,\n",
    "    save_intermediate_results=False,  # Don't save intermediate results\n",
    "    test_alternative_periods=False,\n",
    "    test_alternative_specifications=False\n",
    ")\n",
    "\n",
    "# CPU-optimized configuration (for parallel processing)\n",
    "cpu_optimized_config = HypothesisTestingConfig(\n",
    "    bootstrap_iterations=500,  # Moderate iterations\n",
    "    enable_robustness_tests=True,  # Enable for parallel processing\n",
    "    test_alternative_periods=True,\n",
    "    test_alternative_specifications=True\n",
    ")\n",
    "\n",
    "# Development configuration (fastest)\n",
    "dev_config = HypothesisTestingConfig(\n",
    "    start_date=\"2018-01-01\",\n",
    "    end_date=\"2020-12-31\",\n",
    "    bootstrap_iterations=25,\n",
    "    h1_threshold_trim=0.20,\n",
    "    h2_max_horizon=6,\n",
    "    h2_lags=2,\n",
    "    h2_use_instrumental_variables=False,\n",
    "    h3_causality_lags=2,\n",
    "    h3_test_spillovers=False,\n",
    "    enable_robustness_tests=False,\n",
    "    save_intermediate_results=False\n",
    ")\n",
    "\n",
    "print(\"Performance Configurations:\")\n",
    "print(f\"Memory optimized: {memory_optimized_config.bootstrap_iterations} iterations\")\n",
    "print(f\"CPU optimized: {cpu_optimized_config.bootstrap_iterations} iterations\")\n",
    "print(f\"Development: {dev_config.bootstrap_iterations} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Analysis Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multi-period comparative analysis\n",
    "def run_comparative_analysis(api_key, periods, base_config=None):\n",
    "    \"\"\"\n",
    "    Run analysis across multiple time periods for comparison\n",
    "    \"\"\"\n",
    "    if base_config is None:\n",
    "        base_config = HypothesisTestingConfig()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for period_name, (start_date, end_date) in periods.items():\n",
    "        print(f\"\\nAnalyzing period: {period_name} ({start_date} to {end_date})\")\n",
    "        \n",
    "        # Create period-specific configuration\n",
    "        period_config = HypothesisTestingConfig(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            **{k: v for k, v in base_config.__dict__.items() \n",
    "               if k not in ['start_date', 'end_date']}\n",
    "        )\n",
    "        \n",
    "        # Initialize components\n",
    "        collector = HypothesisDataCollector(fred_api_key=api_key)\n",
    "        tester = QEHypothesisTester(data_collector=collector, config=period_config)\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            data = tester.load_data()\n",
    "            \n",
    "            # Test hypotheses\n",
    "            h1_results = tester.test_hypothesis1(data)\n",
    "            h2_results = tester.test_hypothesis2(data)\n",
    "            h3_results = tester.test_hypothesis3(data)\n",
    "            \n",
    "            results[period_name] = {\n",
    "                'hypothesis1': h1_results,\n",
    "                'hypothesis2': h2_results,\n",
    "                'hypothesis3': h3_results,\n",
    "                'period': (start_date, end_date)\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✓ {period_name} analysis completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {period_name} analysis failed: {str(e)[:100]}...\")\n",
    "            results[period_name] = {'error': str(e), 'period': (start_date, end_date)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (commented out to avoid actual execution)\n",
    "# comparative_periods = {\n",
    "#     'Pre_Crisis': ('2000-01-01', '2007-12-31'),\n",
    "#     'Crisis': ('2008-01-01', '2009-12-31'),\n",
    "#     'QE_Era': ('2010-01-01', '2015-12-31'),\n",
    "#     'Post_QE': ('2016-01-01', '2019-12-31'),\n",
    "#     'COVID': ('2020-01-01', '2023-12-31')\n",
    "# }\n",
    "# \n",
    "# results = run_comparative_analysis('YOUR_API_KEY', comparative_periods, dev_config)\n",
    "\n",
    "print(\"Comparative analysis workflow defined\")\n",
    "print(\"Use run_comparative_analysis() to execute across multiple periods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Sensitivity analysis workflow\n",
    "def run_sensitivity_analysis(api_key, parameter_configs, hypothesis='all'):\n",
    "    \"\"\"\n",
    "    Run sensitivity analysis across different parameter configurations\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for param_type, param_value, config in parameter_configs:\n",
    "        config_name = f\"{param_type}_{param_value}\"\n",
    "        print(f\"\\nTesting {config_name}...\")\n",
    "        \n",
    "        try:\n",
    "            collector = HypothesisDataCollector(fred_api_key=api_key)\n",
    "            tester = QEHypothesisTester(data_collector=collector, config=config)\n",
    "            data = tester.load_data()\n",
    "            \n",
    "            config_results = {}\n",
    "            \n",
    "            if hypothesis in ['all', 'hypothesis1']:\n",
    "                config_results['hypothesis1'] = tester.test_hypothesis1(data)\n",
    "            \n",
    "            if hypothesis in ['all', 'hypothesis2']:\n",
    "                config_results['hypothesis2'] = tester.test_hypothesis2(data)\n",
    "            \n",
    "            if hypothesis in ['all', 'hypothesis3']:\n",
    "                config_results['hypothesis3'] = tester.test_hypothesis3(data)\n",
    "            \n",
    "            results[config_name] = {\n",
    "                'results': config_results,\n",
    "                'parameter': (param_type, param_value),\n",
    "                'config': config\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✓ {config_name} completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {config_name} failed: {str(e)[:100]}...\")\n",
    "            results[config_name] = {\n",
    "                'error': str(e),\n",
    "                'parameter': (param_type, param_value)\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Sensitivity analysis workflow defined\")\n",
    "print(\"Use run_sensitivity_analysis() to test parameter sensitivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configuration Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration validation function\n",
    "def validate_configuration(config):\n",
    "    \"\"\"\n",
    "    Validate configuration settings and provide recommendations\n",
    "    \"\"\"\n",
    "    warnings = []\n",
    "    errors = []\n",
    "    recommendations = []\n",
    "    \n",
    "    # Date validation\n",
    "    start_date = pd.to_datetime(config.start_date)\n",
    "    end_date = pd.to_datetime(config.end_date)\n",
    "    \n",
    "    if start_date >= end_date:\n",
    "        errors.append(\"Start date must be before end date\")\n",
    "    \n",
    "    period_length = (end_date - start_date).days\n",
    "    if period_length < 365:\n",
    "        warnings.append(\"Very short analysis period (< 1 year)\")\n",
    "    elif period_length < 730:\n",
    "        warnings.append(\"Short analysis period (< 2 years)\")\n",
    "    \n",
    "    # Bootstrap validation\n",
    "    if config.bootstrap_iterations < 50:\n",
    "        warnings.append(\"Very few bootstrap iterations may affect reliability\")\n",
    "    elif config.bootstrap_iterations > 2000:\n",
    "        warnings.append(\"Many bootstrap iterations will be slow\")\n",
    "        recommendations.append(\"Consider reducing bootstrap_iterations for faster execution\")\n",
    "    \n",
    "    # Model parameter validation\n",
    "    if config.h1_threshold_trim < 0.05 or config.h1_threshold_trim > 0.4:\n",
    "        warnings.append(\"Unusual threshold trimming parameter\")\n",
    "    \n",
    "    if config.h2_max_horizon > 30:\n",
    "        warnings.append(\"Very long horizon may be unreliable\")\n",
    "    \n",
    "    if config.h2_lags > 8:\n",
    "        warnings.append(\"Many lags may cause overfitting\")\n",
    "    \n",
    "    # Performance recommendations\n",
    "    if (config.bootstrap_iterations > 1000 and \n",
    "        config.enable_robustness_tests and \n",
    "        config.h2_max_horizon > 20):\n",
    "        recommendations.append(\"Configuration may be very slow - consider reducing parameters\")\n",
    "    \n",
    "    # Memory recommendations\n",
    "    if (period_length > 5000 and  # > ~13 years\n",
    "        config.bootstrap_iterations > 500 and\n",
    "        config.save_intermediate_results):\n",
    "        recommendations.append(\"Configuration may use significant memory\")\n",
    "    \n",
    "    return {\n",
    "        'errors': errors,\n",
    "        'warnings': warnings,\n",
    "        'recommendations': recommendations,\n",
    "        'valid': len(errors) == 0\n",
    "    }\n",
    "\n",
    "# Test validation function\n",
    "test_configs = [research_config, prototype_config, dev_config]\n",
    "config_names = ['Research', 'Prototype', 'Development']\n",
    "\n",
    "for name, config in zip(config_names, test_configs):\n",
    "    validation = validate_configuration(config)\n",
    "    print(f\"\\n{name} Configuration Validation:\")\n",
    "    print(f\"  Valid: {validation['valid']}\")\n",
    "    \n",
    "    if validation['errors']:\n",
    "        print(f\"  Errors: {len(validation['errors'])}\")\n",
    "        for error in validation['errors']:\n",
    "            print(f\"    ✗ {error}\")\n",
    "    \n",
    "    if validation['warnings']:\n",
    "        print(f\"  Warnings: {len(validation['warnings'])}\")\n",
    "        for warning in validation['warnings']:\n",
    "            print(f\"    ⚠ {warning}\")\n",
    "    \n",
    "    if validation['recommendations']:\n",
    "        print(f\"  Recommendations: {len(validation['recommendations'])}\")\n",
    "        for rec in validation['recommendations']:\n",
    "            print(f\"    💡 {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated advanced configuration techniques for the QEIR framework:\n",
    "\n",
    "1. **Research vs Prototype Configurations:** Balance between precision and speed\n",
    "2. **Episode-Specific Analysis:** Focus on particular QE periods\n",
    "3. **Alternative Proxies:** Test robustness with different data series\n",
    "4. **Parameter Sensitivity:** Systematic testing of model parameters\n",
    "5. **Performance Optimization:** Memory and CPU-optimized settings\n",
    "6. **Custom Workflows:** Comparative and sensitivity analysis patterns\n",
    "7. **Configuration Validation:** Best practices and error checking\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Start Simple:** Use prototype configurations for initial exploration\n",
    "- **Scale Up:** Move to research configurations for final analysis\n",
    "- **Test Robustness:** Use alternative proxies and parameter sensitivity\n",
    "- **Optimize Performance:** Adjust settings based on computational constraints\n",
    "- **Validate Settings:** Always check configuration validity before running\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Apply these configurations to your specific research questions\n",
    "- Combine with the detailed hypothesis notebooks for comprehensive analysis\n",
    "- Use the validation function to ensure optimal settings\n",
    "- Consider computational resources when choosing configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}